{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q1: Define overfitting and underfitting in machine learning. What are the consequences of each, and how\n",
    "can they be mitigated?**\n",
    "\n",
    " Overfitting occurs when a machine learning model is trained too well on the training data and performs poorly on unseen data. On the other hand, underfitting occurs when the model is not able to capture the underlying patterns in the data and performs poorly on both training and unseen data. The consequences of overfitting are that the model may be too complex, leading to poor generalization and poor performance on new data. The consequences of underfitting are that the model may be too simple, leading to poor accuracy on both training and test data. To mitigate overfitting and underfitting, techniques such as regularization, cross-validation, and increasing training data can be used.\n",
    "\n",
    "**Q2: How can we reduce overfitting? Explain in brief.**\n",
    "\n",
    "To reduce overfitting, one can use techniques such as regularization, early stopping, and dropout. Regularization adds a penalty term to the loss function to reduce the complexity of the model. Early stopping stops training the model when the performance on the validation set starts to degrade. Dropout randomly drops out some neurons during training, forcing the model to learn more robust features.\n",
    "\n",
    "**Q3: Explain underfitting. List scenarios where underfitting can occur in ML.**\n",
    "\n",
    "Underfitting occurs when the model is too simple and is not able to capture the underlying patterns in the data. It can occur when the training data is insufficient, or the model is not complex enough to capture the variability in the data.\n",
    "\n",
    "**Q4: Explain the bias-variance tradeoff in machine learning. What is the relationship between bias and\n",
    "variance, and how do they affect model performance?**\n",
    "\n",
    "The bias-variance tradeoff refers to the tradeoff between model complexity and generalization. Bias measures how well the model fits the training data, while variance measures how much the model's predictions vary with changes in the training data. High bias models are too simple and cannot capture the underlying patterns in the data, while high variance models are too complex and overfit the training data.\n",
    "\n",
    "**Q5: Discuss some common methods for detecting overfitting and underfitting in machine learning models.\n",
    "How can you determine whether your model is overfitting or underfitting?**\n",
    "\n",
    " Common methods for detecting overfitting and underfitting include cross-validation, examining the training and validation error curves, and examining the residuals. To determine whether a model is overfitting or underfitting, one can compare the training and validation error curves. If the training error is much lower than the validation error, the model is overfitting. If both errors are high, the model is underfitting.\n",
    "\n",
    "**Q6: Compare and contrast bias and variance in machine learning. What are some examples of high bias\n",
    "and high variance models, and how do they differ in terms of their performance?**\n",
    "\n",
    "Bias measures how well the model fits the training data, while variance measures how much the model's predictions vary with changes in the training data. High bias models are too simple and cannot capture the underlying patterns in the data, while high variance models are too complex and overfit the training data. An example of a high bias model is a linear regression model with insufficient features, while an example of a high variance model is a deep neural network with too many parameters.\n",
    "\n",
    "**Q7: What is regularization in machine learning, and how can it be used to prevent overfitting? Describe\n",
    "some common regularization techniques and how they work.**\n",
    "\n",
    "Regularization is a technique used to prevent overfitting by adding a penalty term to the loss function to reduce the complexity of the model. Common regularization techniques include L1 regularization, L2 regularization, and dropout. L1 regularization adds a penalty term proportional to the absolute value of the weights, while L2 regularization adds a penalty term proportional to the square of the weights. Dropout randomly drops out some neurons during training, forcing the model to learn more robust features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
